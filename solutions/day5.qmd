---
title: "Lab exercise: day 5"
editor: visual
---

```{r}
#| label: setup

library(tsibble)
library(tsibbledata)
library(fable)
library(feasts)
library(tidyverse)
```

# Learn

Compare the in-sample accuracy statistics for all models used to forecast Australia's print media turnover. Which is most accurate, and which is least? Does this align with your expectations?

```{r}
aus_print <- aus_retail |> 
  filter(Industry == "Newspaper and book retailing") |> 
  summarise(Turnover = sum(Turnover))

aus_print |> 
  autoplot(Turnover)

fit <- aus_print |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

accuracy(fit)
```

> The most accurate model is ARIMA based on RMSE and MAE, the least accurate is SNAIVE.

Modify the code to produce and evaluate forecasts on a test set. Does this change your conclusions?

```{r}
fit_train <- aus_print |> 
  filter(Month < yearmonth("2016 Jan")) |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit_train |> 
  forecast(h = "2 years") |> 
  accuracy(aus_print)
```

> The most accurate model on the 2-year test set is the ARIMA model again, however the SNAIVE model performed much better on this time perios. The linear regression model performed the worst on the test set.

Now compare the cross-validated accuracy statistics for all models used to forecast Australia's print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) and out-of-sample (test set) accuracy?

```{r}
#| warning: false

fit_cv <- aus_print |> 
  stretch_tsibble(.init = 240, .step = 12) |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit_cv |>
  forecast(h = "1 year") |>
  accuracy(aus_print)
```

> The ARIMA model continues to be the best forecasting model for this dataset. On the cross-validated test sets the SNAIVE performs the worst, which suggests that the model was simply lucky on the previous test set.

Produce a residual diagnostics plot of the ETS and ARIMA models, are the residuals consistent with the assumptions of the models? Do they appear random (time plot), uncorrelated (ACF plot), and normally distributed (histogram)?

```{r}
fit <- aus_print |> 
  model(
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit |> 
  select(ets) |> 
  gg_tsresiduals()

fit |> 
  select(arima) |> 
  gg_tsresiduals()
```

> The ETS model's residuals in the time plot look okay, although the ACF reveals that there is still significant autocorrelations in the residuals. The large significant ACF at lag 12 indicates that there still exists substantial seasonality that the model hasn't captured. The histogram seems mostly Normally distributed.
>
> The ARIMA model's residuals in the time plot also look good, and the ACF is much better than the the ETS model's residual ACF. In particular the ACF at lag 12 is not significant, which indicates that it is much better than ETS at capturing the seasonality. The histogram shows a Normal distribution, which implies that the forecast intervals can be trusted.

# Apply

```{r}
library(tsibble)
library(fable)
library(feasts)
library(fabletools)
library(tidyverse)

vaccine_administrated_tsb <- read_rds("data/vaccine_administrated_tsb.rds")
```

## Basic of train/test forecast accuracy

### Split data

We tart by splitting data into two sets to describe the modeling process. We leave out 12 periods (equal to forecast horizon) as test set, and we pretend this is the future we want to forecast.

```{r}
forecast_horizon <- 12# forecast horizon
test <- vaccine_administrated_tsb |> filter(month >= (max(month)-forecast_horizon+1))
train <- vaccine_administrated_tsb |> filter(month < (max(month)-forecast_horizon+1))
```

### Specify and train models

We specify four models with different components and train them on the train part:

```{r}
fit_vaccine <- train |>
  model(
    mean = MEAN(dose_adminstrated),
    naive = NAIVE(dose_adminstrated),
    snaive = SNAIVE(dose_adminstrated),
    automatic_ets = ETS(dose_adminstrated),
    automatic_arima = ARIMA(dose_adminstrated),
  regression1 = TSLM(dose_adminstrated ~ trend() + season()),
  regression_population = TSLM(dose_adminstrated ~ trend() + season() + population_under1),
  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |> 
  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3
)
```

### Produce forecasts

Here we need to first prepare the future values of predictors corresponding to the test set:

#### Values of population and strike in the test set

```{r}
forecast_horizon <- 12
fcs_ets_population <- train |> model(ets=ETS(population_under1)) |> forecast(h=forecast_horizon)

population_forecast <- fcs_ets_population |> as_tibble() |> select(.mean)

test_future <- bind_cols(test,population_forecast) |>
  mutate(dose_adminstrated=.mean) |> 
  select(-.mean,-dose_adminstrated)
```

#### Produce forecasts for dose adminstrated

```{r}
forecast_vaccine <- fit_vaccine |> 
  forecast(new_data = test_future)
```

You can visualise your forecast and see how the forecast looks like visually for the 12 month we predicted:

```{r}
forecast_vaccine |> 
  autoplot(filter_index(train, "2020" ~ .), level=NULL)
```

### Compute forecast accuracy

Let's compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:

```{r}
forecast_vaccine |> 
  accuracy(vaccine_administrated_tsb,
           measures = list(point_accuracy_measures,
                           interval_accuracy_measures, 
                           distribution_accuracy_measures)
)
```

Which error metric do you use to evaluate the performance of models? Which model has the lowest error for each region? discuss your observation.

## Advanced performance evaluation

### Time series cross validation

> Attention: depedning on the umber of time series, the computation time might be big and cause compuationla time issues.

This is also called rolling forecast or rolling origin: You can also reflect on the following questions:

-   Why do we use TSCV? you can read more [her](https://otexts.com/fpp3/tscv.html)

-   How do we do TSCV in R? Which steps to follow?

    1.  split data using `filter_index()` or other functions
    2.  create different time series origins
    3.  model each origin,
    4.  forecast each origin

let's see how we do it in R:

#### split data

We initially split the data into test and train. We defined a new variable, `percentage_test`. This will determine the percentage of the time series we use to evaluate the forecast accuracy using TSCV. As a general rule, we use 20%-30% of the length of time series as the test set. For instance, if we have 120 months of data, and use 20% as test set, that means we will have 24 months (120\*0.2) in the test set:

```{r}
forecast_horizon <- 12# forecast horizon
percentage_test <- 0.2 #20% of time series for test set

test <- vaccine_administrated_tsb |> 
  filter_index(as.character(max(vaccine_administrated_tsb$month)-round(percentage_test*length(unique(vaccine_administrated_tsb$month)))+1) ~ .)

train <- vaccine_administrated_tsb |>
  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(round(percentage_test*length(unique(vaccine_administrated_tsb$month))))))
```

#### Time series cross validation

Before fitting the models, we need to create the time series origins in both train and test sets. We first apply time series cross validation on the train data. We start with an initial training, the length of the first origin (.init = ) and then increase the length of the previous origin by adding new observation (.step=), we continue creating these timeseries until the number of observation left at the end of timeseries equals to the forecast horizon, we stop there.

Next, we apply time series cross validation on the test data. We create slides in the test set that corresponds to each origin created using train data, equal to the length of the forecast horizon.

```{r}
train_tscv <- vaccine_administrated_tsb |> 
  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(forecast_horizon))) |>
  stretch_tsibble(.init = length(unique(train$month)), .step = 1) # split data into different time series (i.e. origin or id) with increasing size

# you need also to get future values that correspond to each .id, because you need them in the forecast model:
test_tscv <- test |> 
  slide_tsibble(.size = forecast_horizon, .step = 1, .id = ".id") |> select(-dose_adminstrated)
```

> `.init` is the size of first origin, `.step` is the increment step, this can correspond to the forecasting frequency, i.e. how often you generate the forecast. If .step = 1 in a monthly time series, it means we generate forecasts very month for the given forecast horizon.

#### Values of population and strike in the test set

> It is important to replace `population_under1` values in the test_tscv with its estimation, otherwise we use perfect forecast for the population_under1 in the models using those predictors which can mislead us in choosing the most accurate model.

We don't have access to these forecast, so here we forecast them using ETS. Complete the R code to produce the estimation of population_under1 abd replace it with actual values in `test_tscv`:

```{r}
fcs_ets_population_tscv <- train_tscv |> 
  model(ets=ETS(population_under1)) |> 
  forecast(h=forecast_horizon)

population_forecast_tscv <- fcs_ets_population_tscv |> as_tibble() |> 
   select(-c(.model,population_under1))

test_future_tscv <-inner_join(population_forecast_tscv,test_tscv) |>
  mutate(population_under1=.mean) |> select(-.mean) |> 
  as_tsibble(index = month, key = c(.id,region))


population_forecast_tscv <- fcs_ets_population_tscv |> 
  as_tibble() |> select(.mean)

test_future_tscv <- bind_cols(test_tscv,population_forecast_tscv) |>
  mutate(population_under1=.mean) |> 
  select(-.mean) |> tsibble::update_tsibble(key = c(.id, region))
```

#### specify and train models

We can train time series cross validation time series with regression models and any other models, this is exactly like what we have done before.

Complete the R code to train all models on the TSCV data:

```{r}
fit_tscv <-  train_tscv |> 
  model(
    mean = MEAN(dose_adminstrated),
    naive = NAIVE(dose_adminstrated),
    snaive = SNAIVE(dose_adminstrated),
    automatic_ets = ETS(dose_adminstrated),
    automatic_arima = ARIMA(dose_adminstrated),
  regression1 = TSLM(dose_adminstrated ~ trend() + season()),
  regression_population = TSLM(dose_adminstrated ~ trend() + season() + population_under1),
  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |> 
  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3
)
fit_tscv
```

Observe the `fit_tscv` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

#### produce forecasts

We can forecast using trained models above. Complete the R code to produce forecasts for the TSCV:

```{r}
fcst_tscv <- fit_tscv |> 
  forecast(new_data = test_future_tscv)
fcst_tscv
```

Observe the `fcst_tscv` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

#### forecast accuracy

Let's compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:

```{r}
fcst_accuracy <- fcst_tscv |> 
  accuracy(vaccine_administrated_tsb,
           measures = list(point_accuracy_measures,
                           interval_accuracy_measures, 
                           distribution_accuracy_measures)
)

fcst_accuracy |> group_by(.model) |> summarise(MASE=mean(MASE), winkler=mean(winkler), CRPS=mean(CRPS)) |> arrange(MASE)
```

Observe the `fcst_accuracy` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

You may want to select a measure you focus on:

```{r}
fcst_accuracy |> select(RMSE,MAE,MASE, winkler, CRPS)
```

You can calculate the overall accuracy across all regions:

```{r}
fcst_accuracy |> group_by(.model) |> summarise(MASE=mean(MASE), winkler=mean(winkler), CRPS=mean(CRPS)) |> arrange(MASE)
```

This will provide an overall summary (i.e an average) of multiple accuracy measures across all origins and forecast horizon. The result is summarised automatically across all origins (.id) and horizon using a simple average.

Which method is the best method (i.e. lowest error metric)?

#### accuracy per id

Now let's see how we can get the accuracy measure for each origin (i.e. .id) separately instead of averaging across all of them. To do this, you need to use an additional argument in accuracy(by=):

```{r}
fc_accuracy_by_id <- fcst_tscv |> 
  accuracy(vaccine_administrated_tsb, by = c(".model", ".id","region"))
```

We can now create some insightful visualisations. Complete the following code to generate a density plot and a box plot that highlights the distribution of the error metrics. You can choose any error metric:

```{r}
# Density plot
fc_accuracy_by_id |> 
  select(.id,.model,MASE) |> 
  ggplot(aes(MASE))+
    geom_density(aes(fill=factor(.model)), alpha=.5)
```

```{r}
#Boxplot
fc_accuracy_by_id |> 
  select(.id,.model,MASE) |> 
ggplot(aes(y= fct_reorder(.model,MASE), x=MASE))+
    geom_boxplot()
```

What insights do these plots provide?

#### accuracy across horizon

What if you want to show the accuracy measure for each model and each horizon (h=1, 2,...,12)?

In fable we don't get automatically a column that corresponds to forecast horizon (h=1,2,3,..., 12). If this is something you are interested in, you can do it yourself, let's first observe the first 24 observations to see the difference later:

```{r}
fcst_tscv[1:24,]
#View(fcst_tscv[1:24,])
```

We first need to group by `id` and `.model` and then create a new variable called `h` and assign `row_number()` to it (you can type ?row_number in your Console to see what this function does, it simply returns the number of row):

```{r}
fc_h <- fcst_tscv |> 
  group_by(.id,.model, region) |> 
  mutate(h=row_number()) |> ungroup()
#View(fc_h[1:24,])# view the first 24 rows of ae_fc and observe h
```

Now check rows from 12 to 24 to see the difference.

To calculate the accuracy measures for each horizon and model, complete the following code :

```{r}
fc_accuracy_h <- fc_h |> 
  as_fable(response = "dose_adminstrated", distribution = "dose_adminstrated") |> 
accuracy(vaccine_administrated_tsb, 
           measures = list(point_accuracy_measures, 
                           interval_accuracy_measures, 
                           distribution_accuracy_measures),
           by = c("region",".model","h"))
```

You can now create a line chart to show how forecast accuracy may change over the forecast horizon. Please complete the R code for a metric of your preference. You can replicate this process by changing the chosen metric:

```{r}
ggplot(data = fc_accuracy_h, 
       mapping = aes(x = h, y = MASE, color = .model))+
         geom_point()+
  geom_line()+
  facet_wrap(vars(region), scales="free_y")+
  ggthemes::scale_color_colorblind()+
  scale_x_continuous(breaks = 1:12)+
ggthemes::theme_clean()+
  labs(x="Month",y="Acuracy")
```

What insights do these plots provide?

### Forecast using best model for the future and visualise it

Now, we need to generate forecast for the future of the time series using the best model identified above. In order to do that, we need to get the values of predictors in the future corresponding to the forecast horizon, we first need to use `new_data()` followed by some data manipulation to get the new data required for forecasting:

You first need to produce the future months. Complete the following code todo that:

```{r}
future_month <- new_data(vaccine_administrated_tsb, n=forecast_horizon)
```

We assume that we know that in March the country will face strikes. Add a new column, *strike* by completing the R code:

```{r}
future_month_strike <- future_month |> 
  mutate(strike = if_else(lubridate::month(month, label = TRUE) == "Mar", 1,0))
```

Add a new column, `population_under1`, to include the estimated population under 1, by completing the R code:

```{r}
forecast_population <- vaccine_administrated_tsb |> model(regression_population=ETS(dose_adminstrated)) |> forecast(h=forecast_horizon)

population_point_forecast <- forecast_population |> as_tibble() |> select(.mean)
```

```{r}
test_future <- bind_cols(future_month_strike, population_point_forecast) |>
  mutate(population_under1=.mean) |> select(-.mean)
```

Train the combination approach on the entire time series data:

```{r}
fit_future <- vaccine_administrated_tsb |> 
  model(
    automatic_ets = ETS(dose_adminstrated),
    automatic_arima = ARIMA(dose_adminstrated),
  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |> 
  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3
) |> select(combination)
```

Forecast using the combination approach for the future:

```{r}
fcst_future <- fit_future |> 
  forecast(new_data = test_future)
```

```{r}
fcst_future |> 
  autoplot(filter_index(vaccine_administrated_tsb, "2020" ~ .))# visualise it
```

### Residual diagnostics

Now, let's perform the residual diagnostic for the most accurate forecasts identified above though time series cross validation.

Plot the residuals:

```{r}
fit_future |> augment() |> filter(region == "A") |> 
autoplot(.resid) +
  labs(title = "Residuals from the from the most accurate model")
```

Create the histogram of residuals:

```{r}
fit_future |> augment() |> filter(region == "A") |>
  ggplot(aes(x = .resid)) +
  geom_histogram() +
  labs(title = "Histogram of residuals from the most accurate model")
```

Create the ACF plot of residuals:

```{r}
fit_future |> augment() |> filter(region == "A") |>
  ACF(.resid) |>
  autoplot() +
  labs(title = "Residuals from the most accurate model")
```

Instead, you could use a function that provides all three plots together:

```{r}
fit_future |> filter(region == "A") |>
  gg_tsresiduals()
```

What does the analysis of residuals reveal about the best model? Are there any systematic patterns left in the residuals?
